{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation\n",
    "\n",
    "We now turn our attention to Retrieval-Augmented Generation (RAG), which is a fundamental part of a lot of LLM applications. RAG consists on using the user query to search a database of information, usually from text documents, that can be provided to the model as context for generating text.\n",
    "\n",
    "## Components of RAG\n",
    "\n",
    "- **Encoding Procedure**\n",
    "\n",
    "To be able to search the most relevant information, it first has to be converted into vectors that encode the semantic contents of each document. This is done through a transformer model that is able to detect the most important parts of a text and turn in into an n-dimensional vector, which each dimension representing an abstract semantic field. In our case, we will be using 1536 dimensions, as that is the output of the ada-2 encoding model provided by OpenAI.\n",
    "\n",
    "- **Knowledge Base**\n",
    "\n",
    "This is the database of information, usually in the form of semantically encoded vectors or **vector store**. It will be used for retrieving information relevant to the user query.\n",
    "\n",
    "- **Retrieval Procedure**\n",
    "\n",
    "By having the documents encoded into vectors and stored, we can perform a **semantic search**, which is the way we can find the most relevant information for each query. The way this is done is by first encoding the query the same way we encoded the documents, such that we obtain an n-dimensional vector with semantic information about the query. The vectors in the Vector Store that most closely resemble the query vector should also be the closest ones in terms of semantic content, and therefore be the most relevant ones. This means we can use simple mathematical operations to assess the similarity stored vectors and the query vector, and keep the most similar ones. A lot of applications use the cosine of the angle between the query vector and the stored vectors, and this is what we will be doing in this demonstration; other metrics can be used, like euclidian distance and dot product between the vectors.\n",
    "\n",
    "## A simple architecture with RAG\n",
    "\n",
    "We can now add retrieval to the previous app design:\n",
    "\n",
    "![rag](_assets/rag.png)\n",
    "\n",
    "What this does is give more specific information to the model for generating responses, which makes for higher quality answers\n",
    "\n",
    "## Limits of RAG\n",
    "\n",
    "There are some details that have to be taken into consideration to use RAG with LLMs. First of all, the quality of the generated responses is, at best, dependant on the quality of the texts. If the texts contain poor information or are poorly composed, this will deteriorate the quality of the responses.\n",
    "\n",
    "Another issue arises from the limits of the LLMs themselves. They are, after all, Neural Networks, and their input layer has a specified amount of nodes. The input that an LLM takes is called a token, which is similar to the text but transformed into numbers, each of which represents a grouping of letters similar to morphemes in language. For the gpt-3.5-turbo-1106 model that we have been using, the maximum amount of tokens that can be passed into the model is 16,385. Therefore, our relevant text must be short enough that it fits withing this limit, together with the rest of the message contents and the chat history. [This website](https://www.tokencounter.io/) helps calculate the amount of tokens that a given text produces. Some documents might wholy fit within the token limit, but longer ones will not. For these situations, we can split the texts into \"chunks\", and the semantic search proceedure will produce the most relevant chunks.\n",
    "\n",
    "## Applying RAG\n",
    "\n",
    "The first general objective we have now is creating a database with the necessary information to answer our queries. To do this, we follow these steps:\n",
    "\n",
    "1. Load the documents and turn them into strings\n",
    "2. Split each document string into smaller chunks\n",
    "3. Encode the chunks\n",
    "4. Add the encoded chunks into a vector store\n",
    "\n",
    "First we will load the necessary libraries: `pypdf` for reading the pdfs and turning them into strings, and `chromadb` for creating and managing the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb pypdf\n",
    "# !pip install pypdf\n",
    "import chromadb\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn each PDF into a string through the `PdfReader` class. This class has an iterable attribute called `pages` that contains all the pages in the document. We have to get the text for each page with the `extract_text` method and then join all the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfToString(path):\n",
    "        loadedPdf = PdfReader(path)\n",
    "        pdfString = \"\"\n",
    "        for page in loadedPdf.pages:\n",
    "                pdfString += page.extract_text()\n",
    "        \n",
    "        return pdfString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using two texts. The first one is Verner and Tirole (2010) on the economics of Open Source software development. The second one is by Vasswani et al. (2017)'s seminal work on language transformers and leveraging attention for efficient language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = pdfToString(\"documents/Verner and Tirole (2010).pdf\")\n",
    "doc2 = pdfToString(\"documents/Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser and Polosukhin (2017).pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our documents as strings of text, we can split them into chunks, each of a certain length. To determine the length we can just use the built in `len` function, but a better way might be to count the tokens themselves. This can be done through the `tiktoken` package and easily integrated into a splitting function with `RecursiveCharacterTextSplitter` provided by the `langchain` package, so we will be loading them both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "from tiktoken import get_encoding\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build a function that tokenizes a text and counts the amount of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14972\n",
      "10236\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_encoding(\"cl100k_base\")\n",
    "def count_tokens(text):\n",
    "\treturn len(tokenizer.encode(text))\n",
    "\n",
    "print(count_tokens(doc1))\n",
    "print(count_tokens(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we count the amount of tokens, both texts can potentially fit within the limit allowed by the model. Therefore we do not need to split them into chunks. Just for demonstration purposes, we still will split them into chunks of about 8192 tokens, which is half the model's limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 8192,\n",
    "        chunk_overlap = 150,\n",
    "        length_function = count_tokens\n",
    ")\n",
    "texts = text_splitter.create_documents(\n",
    "        [doc1, doc2], \n",
    "        metadatas = [\n",
    "                {\"authors\": \"Verner and Tirole\", \"year\": \"2010\"},\n",
    "                {\"authors\": \"Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser and Polosukhin\", \"year\": \"2017\"}\n",
    "        ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma functions locally, but there exist cloud-based solutions for vector database storage like Pinecone. The way Chroma works is by it creates a directory with the index. This directory can be ephemeral and disappear after the program is ran, or it can be persistent and remain in storage for later use.\n",
    "\n",
    "First, we have to embed the documents. We could either do this through the embedding classes provided by the `openai` package. In this case, we will be using the embeddings function class provided by `chromadb`; both packages make a call to the same API endpoint, so the results are the same in either case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from Constants import OPENAI_API_KEY\n",
    "openai_ef = OpenAIEmbeddingFunction(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        model_name=\"text-embedding-ada-002\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will supply this function to the collection, so that both our documents and our queries use this embedding system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromaClient = chromadb.PersistentClient()\n",
    "collection = chromaClient.create_collection(\n",
    "        name=\"my_collection\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "        embedding_function=openai_ef\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the collection has already been created, you can open it with the `get_collection` method. You must supply the embedding function again to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection = chromaClient.get_collection(\n",
    "#         name=\"my_collection\",\n",
    "#         embedding_function=openai_ef\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the documents. When these are provided in the `add` method, they are tokenized and embedded with the function we provided earlier. We must also supply ids for each document, and we may optionally provide metadata for each of them. Our `texts` object has a list of `Document` objects, each with a `page_content` and a `metadata` attributes. We can iterate along this list to provide the information to be added to our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "        documents=[document.page_content for document in texts],\n",
    "        metadatas=[document.metadata for document in texts],\n",
    "        ids=[f\"id{i+1}\" for i in range(len(texts))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query the database. This will return us a dictionary that contains information about the most relevant documents according to the semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"NBER WORKING PAPER SERIES\\nTHE SIMPLE ECONOMICS OF OPEN SOURCE\\nJosh Lerner\\nJean Tirole\\nWorking Paper  7600\\nhttp://www.nber.org/papers/w7600\\nNATIONAL BUREAU OF ECONOMIC RESEARCH\\n1050 Massachusetts Avenue\\nCambridge, MA 02138\\nMarch 2000\\nThe assistance of the Harvard Business School’s California Research Center, and Chris Darwall in particular,\\nwas instrumental in the development of the case studies and is gratefully acknowledged.  We also thank anumber of practitioners—especially Eric Allman, Mike Balma, Brian Behlendorf, Keith Bostic, TimO’Reilly, and Ben Passarelli—for their willingness to generously spend time discussing the open sourcemovement.  Jacques Crémer, Bernard Salanié, and Rob Merges provided helpful comments.  HarvardBusiness School’s Division of Research provided financial support.  The Institut D'Economie Industriellereceives research grants from a number of corporate sponsors, including Microsoft Corporation.  All opinionsand errors, however, remain our own. \\n© 2000 by Josh Lerner and Jean Tirole.  All rights reserved.  Short sections of text, not to exceed two\\nparagraphs, may be quoted without explicit permission provided that full credit, including © notice, is givento the source.\\nThe Simple Economics of Open SourceJosh Lerner and Jean Tirole\\nNBER Working Paper No. 7600March 2000JEL No. L22, L31, L86\\nABSTRACT\\nThere has been a recent surge of interest in open source software development, which\\ninvolves developers at many different locations and organizations sharing code to develop and refineprograms.  To an economist, the behavior of individual programmers and commercial companiesengaged in open source projects is initially startling. This paper makes a preliminary exploration ofthe economics of open source software. We highlight the extent to which labor economics, especiallythe literature on “career concerns,” can explain many of these projects’ features.  Aspects of thefuture of open source development process, however, remain somewhat difficult to predict with “off-the-shelf” economic models.\\nJosh Lerner Jean Triole\\nHarvard Business School          Institut D'Economie Indutrielle (IDEI)Morgan Hall, Room 395 Manufacture des Tabacs - MF529Boston, MA  02163,   21 Allées de Brienneand NBER 31000 Toulouse Cedex FRANCEjlerner@hbs.edu tirole@cict.fr1.  Introduction \\nOver the past two years, there has been a surge of interest in open source software \\ndevelopment.  Interest in this process, which involves software developers at many different locations and organizations sharing code to develop and refine software programs, has been spurred by three factors: \\n• The rapid diffusion of open source software .  A number of open source products, \\nsuch as the Apache web server, dominate their product categories.  In the personal \\ncomputer operating system market, International Data Corporation estimates that the open source program Linux has between seven to twenty-one million users worldwide, with a 200% annual growth rate.  Many observers believe it represents a leading potential challenger to Microsoft Windows in this important market segment. \\n• The significant capital investments in open source projects .  Over the past two \\nyears, numerous major corporations, including Hewlett Packard, IBM, and Sun, \\nhave launched projects to develop and use open source software.  Meanwhile, a number of companies specializing in commercializing Linux, such as Red Hat and VA Linux, have completed initial public offerings, and other open source companies such as Cobalt Networks, Collab.Net, Scriptics, and Sendmail have received venture capital financing.  \\n• The new organization structure .  The collaborative nature of open source software \\ndevelopment has been hailed in the business and technical press as an important \\norganizational innovation.   2 Yet to an economist, the behavior of individual programmers and commercial \\ncompanies engaged in open source processes is startling. Consider these quotations by two leaders of the open source community: \\nThe idea that the proprietary software social system—the system that says \\nyou are not allowed to share or change software—is unsocial, that it is unethical, that it is simply wrong may come as a surprise to some people.  But what else can we say about a system based on dividing the public and keeping users helpless? [Stallman, 1999]   The “utility function” Linux hackers is maximizing is not classically economic, but is the intangible of their own ego satisfaction and reputation among other hackers. [Parenthetical comment deleted] Voluntary cultures that work this way are actually not uncommon; one other in which I have long participated is science fiction fandom, which unlike hackerdom explicitly recognizes “egoboo” (the enhancement of one’s reputation among other fans) [Raymond, 1999b]. \\n It is not initially clear how these claims relate to the traditional view of the innovative \\nprocess in the economics literature.  Why should thousands of top-notch programmers contribute freely to the provision of a public good?  Any explanation based on altruism\\n1 \\nonly goes so far.  While users in less developed countries undoubtedly benefit from access to free software, many beneficiaries are well-to-do individuals or Fortune 500 companies.  Furthermore, altruism has not played a major role in other industries, so it would have to be explained why individuals in the software industry are more altruistic than others.  \\nThis paper seeks to address this puzzle, by making a preliminary exploration of the \\neconomics of open source software.  Reflecting the early stage of the field’s development, we do not seek to develop new theoretical frameworks or to statistically analyze large samples.  Rather, we focus on three “mini-cases” of particular projects: \\n                                                           \\n1 The media like to portray the open source community as wanting to help mankind, as it makes a good \\nstory.  Many open source advocates put limited emphasis on this explanation.  3 Apache, Perl, and Sendmail.2  We seek to draw some initial conclusions about the key \\neconomic patterns that underlie the open source development of software.  We find that much can be explained by reference to economic frameworks.  We highlight the extent to which frameworks of labor economics, and in particular the literature on “career concerns,” can explain many of the features of open source projects. \\nAt the same time, we acknowledge that aspects of the future of open source \\ndevelopment process remain somewhat difficult to predict with “off-the-shelf” economic models.  In the final section of this paper, we highlight a number of puzzles that the movement poses.  It is our hope that this section will have itself an “open source” nature: that it will stimulate research by other economic researchers as well. \\nFinally, it is important to acknowledge the relationship with the earlier literature on \\ntechnological innovation.  The open source development process is somewhat reminiscent of the type of “user-driven innovation” seen in many other industries.  Among other examples, Rosenberg’s [1976] studies of the machine tool industry and von Hippel’s [1988] of scientific instruments have highlighted the role that sophisticated users can play in accelerating technological progress.  In many instances, solutions developed by particular users for individual problems have become more general solutions for wide classes of users.  But as we shall argue below, certain aspects of the open source process—especially the extent to which contributors’ work is recognized and rewarded—are quite distinct from earlier settings.  \\n2. The Nature of Open Source Software \\n                                                           \\n2 These are summarized in Darwall and Lerner [2000].   4 While media attention to the phenomenon of open source software has been recent, \\nthe basic behaviors are much older in their origins.  There has long been a tradition of sharing and cooperation in software development.  But in recent years, both the scale and formalization of the activity have expanded dramatically with the widespread diffusion of the Internet.\\n3  In the discussion below, we will highlight three distinct eras of cooperative \\nsoftware development.   2.1 The first era: early 1960s to the early 1980s .   \\nMany of the key aspects of the computer operating systems and the Internet were \\ndeveloped in academic settings such as Berkeley and MIT during the 1960s and 1970s, as well as in central corporate research facilities where researchers had a great deal of autonomy (such as Bell Labs and Xerox’s Palo Alto Research Center).  In these years, the sharing by programmers in different organizations of basic operating code of computer programs—the source code—was commonplace.\\n4   \\nMany of the cooperative development efforts in the 1970s focused on the development \\nof an operating system that could run on multiple computer platforms.  The most successful examples, the Unix operating system and the C language used for developing Unix applications, were originally developed at AT&T’s Bell Laboratories.  The software was then installed across institutions, being transferred freely or for a nominal charge.  Many of the sites where the software was installed made further innovations, which were \\n                                                           \\n3 This history is of necessity highly abbreviated.  For more detailed treatments, see Browne [1999], \\nDiBona, Ockman, and Stone [1999], Gomulkiewicz [1999], Levy [1984], and Raymond [1999a].   \\n4 Programmers write source code using languages such as Basic, C, and Java.  By way of contrast, most \\ncommercial software vendors only provide users with object, or binary, code.  This is the sequence of 0s \\nand 1s that directly communicates with the computer, but which is difficult for programmers to interpret or modify.  When the source code is made available to other firms by commercial developers, it is typically \\nlicensed under very restrictive conditions. \\n   5 in turn shared with others.  The process of sharing code was greatly accelerated with the \\ndiffusion of Usenet, a computer network begun in 1979 to link together the Unix programming community.  As the number of sites grew rapidly (e.g., from 3 in 1979 to 400 in 1982), the ability of programmers in university and corporate settings to rapidly share technologies was considerably enhanced.  \\nThese cooperative software development projects were undertaken on a highly \\ninformal basis.  Typically no effort to delineate property rights or to restrict reuse of the software were made.  This informality proved to be problematic in the early 1980s, when AT&T began enforcing its (purported) intellectual property rights related to Unix.   2.2 The second era: early 1980s to the early 1990s .   \\nIn response to these threats of litigation, the first efforts to formalize the ground rules \\nbehind the cooperative software development process emerged.  This ushered in the second era of cooperative software development.  The critical institution during this period was the Free Software Foundation, begun by Richard Stallman of the MIT Artificial Intelligence Laboratory in 1983.  The foundation sought to develop and disseminate a wide variety of software without cost.   \\nOne important innovation introduced by the Free Software Foundation was a formal \\nlicensing procedure that aimed to preclude the commercialization of cooperatively developed software.  In exchange for being able to use and modify the GNU software (as it was known), users had to agree to make the source code freely available (or at a nominal cost).  As part of the General Public License (GPL, also known as “copylefting”), the user had to also agree not to impose licensing restrictions on others.  Furthermore, all enhancements to the code—and even code that intermingled the  6 cooperatively developed software with that developed separately—had to be licensed on \\nthe same terms.  It is these contractual terms that distinguish open source software from shareware (where the binary files but not the underlying source code are made freely available, possibly for a trial period only) and public-domain software (where no restrictions are placed on subsequent users of the source code).\\n5 \\nThis project, as well as contemporaneous efforts, also developed a number of \\nimportant organizational features.  In particular, these projects employed a model where contributions from many developers were accepted (and frequently publicly disseminated or posted).  The right to modify the official version of the program, however, was confined to a smaller subset of individuals closely involved with the project, or in some cases, an individual leader.  In some cases, the project’s founder (or his designated successor) served as the leader; in others, leadership rotated between various key contributors. 2.3 The third era: early 1990s to today.  \\n The widespread diffusion of Internet access in the early 1990s led to a dramatic \\nacceleration of open source activity.  The volume of contributions and diversity of contributors expanded sharply, and numerous new open source projects emerged, most notably Linux (a variant of the UNIX operating system developed by Linus Torvalds in 1991).  As discussed in detail below, interactions between commercial companies and the open source community also became commonplace in the 1990s.   \\n                                                           \\n5 It should be noted, however, that some projects, such as the Berkeley Software Distribution (BSD) effort, \\ndid take alternative approaches during the 1980s.  The BSD license is much less constraining than the GPL: anyone can modify the program and redistribute it for a fee without making the source code freely \\navailable.  In this way, it was a continuation of the university-based tradition of the 1960s and 1970s. \\n  7 Another innovation during this period was the proliferation of alternative approaches \\nto licensing cooperatively developed software.  During the 1980s, the GPL was the dominant licensing arrangement for cooperatively developed software.  This changed considerably during the 1990s.  In particular, Debian, an organization set up to disseminate Linux, developed the “Debian Social Contract” in 1995.  This agreement allowed licensees greater flexibility in using the program, including the right to bundle the cooperatively developed software with proprietary code.  This more flexible licensing arrangement was adopted in early 1997 by a number of individuals involved in cooperative software development, and was subsequently known as the “Open Source Definition.”  As the authors explained: \\nLicense Must Not Contaminate Other Software.  The license must not \\nplace restrictions on other software that is distributed along with the licensed software.  For example, the license must not insist that all other programs distributed on the same medium must be open-source software.  Rationale:  Distributors of open-source software have the right to make their own choices about their own software [Open Source Initiative, 1999].  \\nUnlike the General Public License, this new license is not “viral”: it does not “infect” all \\ncode that was bundled with the software with the requirement that it be covered under the license agreement as well. \\nThe past few years have seen unprecedented growth of open source software.  At the \\nsame time, the movement has faced a number of challenges.  We will highlight two of these here: the “forking” of projects (the development of competing variations) and the development of products for high-end users. \\nOne issue that has emerged in a number of open source projects is the potential for \\nprograms splintering into various variants.  In some cases, passionate disputes over product design have led to the splintering of open source projects into different variants.   8 Examples of such splintering are the Berkeley Unix program and Sendmail during the \\nlate 1980s.   \\nAnother challenge has been the apparently lesser emphasis on documentation and \\nsupport, user interfaces,6 and backward compatibility found in at least some open source \\nprojects.  The relative technological features of software developed in open source and traditional environments are a matter of passionate discussion.  Some members of the community believe that this production method dominates traditional software development in all respects.  But many open source advocates argue that open source software tends to be geared to the more sophisticated users.\\n7  This point is made \\ncolorfully by one open source developer: \\n[I]n every release cycle Microsoft always listens to its most ignorant \\ncustomers .  This is the key to dumbing down each release cycle of \\nsoftware for further assaulting the non-personal computing population.  Linux and OS/2 developers, on the other hand, tend to listen to their smartest customers…  The good that Microsoft does in bringing \\ncomputers to non-users is outdone by the curse that they bring on experienced users [Nadeau, 1999].  \\n Certainly, the greatest diffusion of open source projects appears to be in settings where \\nthe end users are sophisticated, such as the Apache server installed by systems administrators.  In these cases, users are apparently more willing to tolerate the lack of detailed documentation or easy-to-understand user interfaces in exchange for the cost savings and the possibility of modifying the source code themselves.  In several projects, such as Sendmail, project administrators chose to abandon backward compatibility in the \\n                                                           \\n6 Two main open source projects (GNOME and KDE) are meant to remedy Linux's handicap on the \\ndesktop (mouse and windows interfaces).  \\n7 For example, Torvalds [1999] argues that the Li nux model works best with developer-type software.  \\nGhosh [1999] views the open source process as a large repeated game process of give-and-take among \\ndeveloper-users (the “cooking pot” model). \\n  9 interests of preserving program simplicity.  One of the rationales for this decision was \\nthat administrators using the Sendmail system were responsive to announcements that these changes would be taking place, and rapidly upgraded their systems.  In a number of commercial software projects, it has been noted, these types of rapid responses are not as common.  Once again, this reflects the greater sophistication and awareness of the users of open source software. \\nThe debate about the ability of open source software to accommodate high-end users' \\nneeds has direct implications for the choice of license.  The recent popularity of more liberal licenses such as the Debian Social Contract and the Open Source Definition and the concomitant decline of the GNU license are related to the rise in the “pragmatists’” influence.  These individuals believe that allowing proprietary code and for-profit activities in segments that would otherwise be poorly served by the open-source community will provide the movement with its best chance for success.  \\n3. The Origins of the Three Programs \\nEach of the three case studies was developed through the review of printed materials \\nand interviews (as well as those posted on various web sites) and face-to-face meetings with one or more key participants in the development effort.  In addition, we held a number of conversations with knowledgeable observers of the open source movement.  In Sections 4 and 5, we will frequently draw on examples from the three cases.  Nonetheless, we felt it would be helpful to first provide a brief overview of the three development projects.  10 3.1 Apache. \\n The development of Apache began in 1994.  Brian Behlendorf, then 21, had the \\nresponsibility for operating one of the first commercial Internet servers in the country, that powering Wired  magazine’s HotWired web site.  This server, like most others in the \\ncountry, was at the time running the Unix-based software written at the National Center for Supercomputer Applications (NCSA) at the University of Illinois.  (The only competitive product at the time was the server developed at the joint European particle physics research facility CERN.)  The NCSA had distributed its source code freely and had a development group actively involved in refining the code in consultation with the pioneering users.  As Behlendorf and other users wrote emendations, or “patches,” for the NCSA server, they would post them as well to mailing lists of individuals interested in Internet technology. \\nBehlendorf and a number of other users, however, encountered increasing frustrations \\nin getting the NCSA staff to respond to their suggestions.  (During this time, a number of the NCSA staff had departed to begin Netscape, and the University was in the process of negotiating a series of licenses of its software with commercial companies.)  As a result, he and six other pioneering developers decided to establish a mailing list to collect and integrate the patches to the NCSA server software.  They agreed that the process would be a collegial one.  While a large number of individuals would be able to suggest changes, only a smaller set would be able to actually make changes to the physical code.  In August 1995, the group released Apache 0.8, which represented a substantial departure from earlier approaches.  A particular area of revision was the Application Program Interface (API), which allowed the development of Apache features to be very  11 “modular.”  This step enabled programmers to make contributions to particular areas \\nwithout affecting other aspects of the programs.   \\nThe diffusion of Apache has been quite dramatic.  Periodic surveys by Netcraft show \\nthat the share of publicly observable Web servers ( i.e., those not behind firewalls) \\nrunning Apache rose from 31% in April 1996 to 44% in June 1997 to 55% in September 1999.\\n8  In 1999, the Apache Software Foundation was established to oversee the \\ndevelopment and diffusion of the program.  The current status of Apache, as well as the other two open source projects that we focused on, is summarized in Table 1 .   \\n3.2 Perl. \\nPerl, or the Practical Extraction and Reporting Language, was created by Larry Wall \\nin 1987.  Wall, a programmer with Burroughs (a computer mainframe manufacturer now part of Unisys) had already written a number of widely adopted software programs.  These included a program for reading postings on on-line newsgroups and a program that enabled users to readily update old source code with new patches. \\nThe specific genesis of Perl was the large number of repetitive system administration \\ntasks that Wall was asked to undertake while at Burroughs.  In particular, Wall was required to synchronize and generate reports on two Unix-based computers as part of a project that Burroughs was undertaking for the U.S. National Security Agency.  He realized that there was a need for a program language that was somewhere between the Unix shell language and the C language (suitable for developing complex programming applications).  The Perl language sought to enable programmers to rapidly undertake a \\n                                                           \\n8 A complication is introduced by the fact that firewall-protected servers may be quite different in nature.  \\nFor instance, a survey of both protected and unprotected servers in the summer of 1996 by Zoma Research concluded that open source server programs, including Apache, accounted for only 7% of all installations, far less than the contemporaneous Netcraft estimate.  12 wide variety of system administration tasks.  The program was first introduced in 1987 \\nvia the Internet.  It has become widely accepted as a language for developing scripts for Apache web servers, and is incorporated in a number of other programs.   \\nPerl is administered on a rotating basis: the ten to twenty programmers (the number \\nfluctuates over time) who have been most actively involved in the program take turns managing different aspects of the project.  Wall himself has joined the staff of O’Reilly & Associates, a publisher specializing in manuals documenting open source programs.  While he is no longer actively contributing to the programming, he remains active in managing the project. \\nTwo efforts to establish a Perl-related foundation have foundered.  The Perl Institute \\nhad been intended to ensure that less glamorous tasks, such as documentation, were undertaken, in order to enhance the long-run growth of Perl.  The failure of these efforts may have reflected more about the specifics of the individual personalities involved than the prospects of the program itself. 3.3 Sendmail. \\nSendmail was originally developed in the late 1970s by Eric Allman, a graduate \\nstudent in computer science at the University of California at Berkeley.  As part of his responsibilities, Allman worked on a variety of software development and system administration tasks at Berkeley.   \\nOne of the major challenges that Allman faced was the incompatibility of the two \\nmajor computer networks on campus.  The approximately one dozen Unix-based computers had been originally connected through “BerkNet,” a locally developed program that provided continuous interconnection.  These computers, in turn connected \\n                                                                                                                                                                             \\n  13 to those on other campuses through telephone lines, using the UUCP protocol (Unix-to-\\nUnix Copy Protocol).  Finally, the Arpanet, the direct predecessor to the Internet, was introduced on the Berkeley campus around this time.  Each of the networks used a different communications protocol: for instance, each person had multiple e-mail addresses, depending on the network from which the message was sent.  To cope with this problem, Allman developed in 1979 a program called “Delivermail,” which provided a way to greatly simplify the addressing problem.  In an emendated form that allowed it to address a large number of domains, it was released two years later as “Sendmail.” \\nSendmail was soon adopted as the standard method of routing e-mail on the Arpanet.  \\nAs the network grew, however, its limitations became increasingly apparent.  A variety of enhanced versions of Sendmail were released in the 1980s and early 1990s which were incompatible with each other—in the argot of the open source community, the development of the program “forked.”  In 1993, Allman, who had returned to working at Berkeley after being employed at a number of software firms, undertook a wholesale rewrite of Sendmail.  The development was sufficiently successful that the incompatible versions were largely abandoned in favor of the new version.  By 1998, it was estimated that 80% of all e-mail traffic was sent by Sendmail. \\nIn 1997, Allman established Sendmail, Inc.  The company, which has been financed \\nby a leading venture capital group Benchmark Capital, is seeking to sell Sendmail-related software enhancements (such as more user-friendly interfaces) and services.  At the same time, the company seeks to encourage the continuing development of the software on an open source basis.  For instance, Sendmail, Inc. employs two engineers who work almost  14 full time on contributions to the open source program, which is run by the non-profit \\nSendmail Consortium.  \\n4.  What Does Economic Theory Tell Us about Open Source? \\n4.1  What motivates programmers?  Theory. \\nA programmer participates in a project, whether commercial or open source, only if \\nshe derives a net benefit (broadly defined) from engaging in the activity.  The net benefit is equal to the immediate  payoff (current benefit minus current cost) plus the delayed  \\npayoff. \\nA programmer working on a software development project incurs a variety of \\nimmediate benefits and costs.  First, the programmer receives monetary compensation if she is working for a commercial firm.  Second, the programmer may be fixing a bug or customizing a program for her own benefit (as well as, in the case of an open source process, for the benefit of others.)  Third, the programmer incurs an opportunity cost of her time.  While she is working on this project, she is unable to engage in another programming activity.  The actual cost of this time depends on how enjoyable the work is.  \\nThe delayed reward covers two distinct, although hard-to-distinguish, incentives.  The \\ncareer concern incentive  refers to future job offers, shares in commercial open source-\\nbased companies,\\n9 or future access to the venture capital market.  The ego gratification \\nincentive  stems from a desire for peer recognition.  Probably most programmers respond \\n                                                           \\n9 Linus Torvalds and others have been awarded shares in Linux-based companies that went public.  Most \\ncertainly, these rewards were unexpected and did not affect the motivation of open source programmers.  If \\nthis practice becomes “institutionalized,” such rewards will in the future be expected and therefore impact \\nthe motivation of open source leaders.  15 to both incentives.  There are some differences between the two.  The programmer \\nmainly preoccupied by peer recognition may shun future monetary rewards, and may also want to signal her talent to a slightly different audience than those motivated by career concerns.  From an economic perspective, however, the incentives are similar in most respects.  We will group the career concern incentive and the ego gratification incentive under a single heading: the signaling incentive . \\nEconomic theory [e.g., Holmström, 1999] suggests that this signaling incentive is \\nstronger, a) the more visible the performance to the relevant audience (peers, labor market, \\nventure capital community), \\nb) the higher the impact of effort on performance, and c) the more informative the performance about talent. \\nThe first condition gives rise to what economists call “strategic complementarities.”  \\nTo have an “audience,” programmers will want to work on software projects that will attract a large number of other programmers.  This suggests the possibility of multiple equilibria.  The same project may attract few programmers because programmers expect that other programmers will not be interested; or it may flourish as programmers (rationally) have faith in the project.   \\nThe same point applies to forking in a given open source project.  Open source \\nprocesses are in this respect quite similar to academic research.  The latter is well known to exhibit fads.  Fields are completely neglected for years, while others with apparently no superior intrinsic interest attract large numbers of researchers.  Fads in academia are frowned upon for their inefficient impact on the allocation of research.  It should not be  16 ignored, however, that fads also have benefits.  A fad can create a strong signaling \\nincentive: researchers working in a popular area may be highly motivated to produce a high-quality work, since they can be confident that a large audience will examine their work. 4.2  Comparison between open source and closed source programming incentives . \\nTo compare programmers' incentives in the open source and proprietary settings, we \\nneed to examine how the fundamental features of the two environments shape the incentives just reviewed.  We will first consider the relative short-term rewards, and then turn to the deferred compensation. \\nCommercial projects have an edge on the current-compensation dimension because \\nthe proprietary nature of the code generates income.  This makes it privately worthwhile for private companies to offer salaries.\\n10  This contention is the old argument in \\neconomics that the prospect of profit encourages investment, which is used, for instance, to justify the awarding of patents to encourage invention.  \\nBy way of contrast,  an open source project may well lower the cost for the \\nprogrammer, for two reasons:  \\ni) “Alumni effect”: Because the code is freely available to all, it can be used in \\nschools and universities for learning purposes; so it is already familiar to \\nprogrammers. This reduces their cost of programming for UNIX, for example.11 \\n                                                           \\n10 To be certain, commercial firms (e.g., Netscape, Sun, O'Reilly, Transmeta) supporting open source \\nprojects are also able to compensate programmers, because they indirectly benefit financially from these \\nprojects.  Similarly,  the government and not-for-profit corporations have done some subsidizing of open source projects.  Still, there should be an edge for commercial companies.\\n  \\n \\n11 WhiIe we are here interested in private incentives to participate, note that this complementarity between \\napprenticeship and projects is socially beneficial.  The social benefits might not increase linearly with open \\nsource market share, however, since the competing open source projects may end up competing for \\nattention in the same common pool of students.  17 ii) Customization and bug-fixing benefîts: The cost of contributing to an open source \\nproject is lower if the activity brings about a private benefit (bug fixing, customization) for the programmer and her firm. Note again that this factor of cost reduction is directly linked to the openness of the source code. \\nLet us now turn to the delayed reward (signaling incentive) component.  In this respect \\ntoo, the open source process has some benefits over the closed source approach.  As we noted, signaling incentives are stronger, the more visible the performance and the more attributable the performance to a given individual.  Signaling incentives therefore may be stronger in the open source mode for three reasons: i) Better performance measurement : Outsiders can only observe inexactly the \\nfunctionality and/or quality of individual elements of a typical commercially developed program, as they are unable to observe the proprietary source code.  By way of contrast, in an open source project, the outsiders are able to see not only what the contribution of each individual was and whether that component “worked,” but also whether the task was hard, if the problem was addressed in a \\nclever way, whether the code can be useful for other programming tasks in the \\nfuture, and so forth. \\nii) Full initiative: The open source programmer is her own boss and takes full \\nresponsibility for the success of a subproject. In a hierarchical commercial firm, however, the programmer's performance depends on her supervisor's interference,  advice, etc.  Economic theory would predict that the programmer's performance is \\nmore precisely measured in the former case.  18 iii) Greater fluidity:  It may be argued that the labor market is more fluid in an open \\nsource environment.  Programmers are likely to have less idiosyncratic, or firm-specific, human capital that limits shifting one’s efforts to a new program or work environment.  (Since many elements of the source code are shared across open source projects, more of the knowledge they have accumulated can be transferred to the new environment).  \\nThese theoretical arguments also provide insights as to who is more likely to \\ncontribute and what tasks  are best suited to open source projects.  Sophisticated users \\nderive direct benefits when they customize or fix a bug in open source software.\\n12 A \\nsecond category of potential contributors consists of individuals with strong signaling incentives; these may use open source software as a port of entry.  For instance, open source processes may give a talented system administrator at a small academic institution (who is also a user!) a unique opportunity to signal her talent to peers, prospective employers, and the venture capital community.\\n13 \\n                                                           \\n12 A standard argument in favor of open source processes is their massive parallel debugging.  Typically, \\ncommercial software firms can only ask users to point at problems: beta testers do not fix the bugs, they just report them.  It is also interesting to note that many commercial companies do not discourage their employees from working on open source projects.  In many cases where companies encourage such involvement, programmers use open source tools to fix problems.  Johnson [1999] builds a model of open \\nsource production by a community of user-developers.  There is one software program or module to be \\ndeveloped, which is a public good for the potential developers.  Each of the potential developers has a private cost of working on the project and a private value of using it; both of which are private information.  Johnson shows that the probability that the innovation is made need not increase with the number of developers, as free-riding is stronger when the number of potential developers increases.  \\n13 An argument often heard in the open source community is that people participate in open source projects \\nbecause programming is fun and because they want to be “part of a team.”  While this argument may contain a grain of truth, it is puzzling as it stands; for, it is not clear why programmers who are part of a commercial team could not enjoy the same intellectual challenges and the same team interaction as those engaged in open source development.  The argument may reflect the ability of programmers to use participation in open source projects to overcome labor market rigidities that make signaling in other ways \\nproblematic. \\n  19 As to the tasks that may appeal to the open source community, one would expect that \\ntasks such as those related to the operating systems and programming languages, whose natural audience is the community of programmers, would give rise to strong signaling incentives.  By way of contrast, tasks aiming at helping the much-less-sophisticated end user—e.g., documentation, design of easy-to-use interfaces, technical support, and insuring backward compatibility—usually provide lower signaling incentives.\\n14 \\n4.3  Evidence on individual incentives . \\nA considerable amount of evidence is consistent with an economic perspective. First, user benefits are key to a number of open source projects.  One of the origins of \\nthe free software movement was Stallman's inability to improve a printer program because Xerox refused to release the source code.  In each of the three scenarios described in section 3, the project founders were motivated by information technology problems that they had encountered in their day-to-day work.  For instance, in the case of Apache, the initial set of contributors was almost entirely system administrators who were struggling with the same types of problems as Behlendorf.  In each case, the initial release was “runnable and testable”: it provided a potential, even if imperfect, solution to a problem that was vexing considerable numbers of data processing professionals. \\nSecond, it is clear that giving credit to authors is essential in the open source \\nmovement.  This principle is included as part of the nine key requirements in the “Open Source Definition” [Open Source Initiative, 1999].  This point is also emphasized by \\n                                                           \\n14 Valloppillil [1998] further argues that reaching commercial grade quality often involves unglamorous \\nwork on power management, management infrastructure, wizards, etc., that makes it unlikely to attract \\nopen source developers. \\n  20 Raymond [1999b], who points out “surreptitiously filing someone’s name off a project is, \\nin cultural context, one of the ultimate crimes.” \\nFinally, the reputational benefits that accrue from successful contributions to open\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.query(query_texts=[\"what incentives exist in the open source development economy?\"], n_results=2)[\"documents\"][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finished preparing our knowledge base, our next goal will be to build an application that finds the most relevant document for our query, provides it to the language model, and generates a reponse with the information provided. The steps to do this are very similar to the basic application we created previously, with the added step of searching the index. First, we must create a system prompt that reflects this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_delimiter = \"####\"\n",
    "context_delimiter = \"++++\"\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are an assistant tasked with responding to a user's query. For this end, you will be supplied a fragment of an academic paper that is relevant to the user's query.\n",
    "\n",
    "The query will be delimited by {query_delimiter} characters\n",
    "\n",
    "The academic paper fragment will be delimited by {context_delimiter} characters\n",
    "\n",
    "Your answer must only rely on information provided by the academic paper fragment provided\n",
    "\n",
    "If the fragment is not relevant or does not contain the necessary information, make sure to reflect that in your answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a function that takes the query, performs a semantic search, adds the most relevant document to the user message content, and makes an API request to generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "model = \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "def ragProcedure(query, n_results = 1):\n",
    "        relevantText = collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=n_results\n",
    "        )['documents'][0][0]\n",
    "        userMessageContent = f\"{query_delimiter}{query}{query_delimiter}\\n\\n{context_delimiter}{relevantText}{context_delimiter}\"\n",
    "        messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": userMessageContent}\n",
    "        ]\n",
    "        response = client.chat.completions.create(\n",
    "                messages = messages,\n",
    "                model = model\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The academic paper \"The Simple Economics of Open Source\" by Josh Lerner and Jean Tirole explores the economics of open source software development. It discusses motivations and incentives for programmers in both open source and closed source environments. The paper suggests that programmers in open source projects may be motivated by career concerns, ego gratification, and signaling incentives, which can include peer recognition, future job opportunities, and shares in commercial open source-based companies. The visibility of performance, the measurability of effort, and the fluidity of the labor market in open source environments may enhance the signaling incentives for programmers.\n",
      "\n",
      "While the paper provides insights into the motivations and incentives of programmers in the open source development economy, it does not specifically outline a list of incentives existing in the open source development economy. Therefore, it does not directly provide a clear, concise list of incentives. Instead, the paper presents a theoretical framework for understanding the motivations of open source developers.\n",
      "\n",
      "If you need a clear list of incentives existing in the open source development economy, I recommend reviewing other literature or resources specifically addressing this topic.\n"
     ]
    }
   ],
   "source": [
    "print(ragProcedure(\"what incentives exist in the open source development economy?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The academic paper fragment you provided is titled \"Attention Is All You Need\" by Ashish Vaswani et al. In this paper, the authors introduce the Transformer model, which is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The paper discusses how the Transformer model achieves superior performance in machine translation tasks by relying on self-attention to draw global dependencies between input and output, without using sequence-aligned recurrent neural networks (RNNs) or convolution.\n",
      "\n",
      "The comparison between transformers, convolution, and recurrence is addressed in the paper:\n",
      "- The authors propose the Transformer model as an alternative to complex recurrent or convolutional neural networks, highlighting its superiority in terms of quality, parallelizability, and training time.\n",
      "- They indicate that recurrent models factor computation along the symbol positions of the input and output sequences, inherently leading to sequential nature and limiting parallelization within training examples, whereas the Transformer allows for significantly more parallelization.\n",
      "- Furthermore, the paper discusses the computational complexity, the amount of computation that can be parallelized, and the length of the path between long-range dependencies in the network for different layer types, such as self-attention, recurrent layers, and convolutional layers.\n",
      "\n",
      "In conclusion, the academic paper provides a detailed analysis of how the Transformer model compares to convolution or recurrence, highlighting its advantages in terms of parallelization, computational complexity, and effective resolution. It explains why self-attention in the Transformer model improves long-range dependency learning and provides superior performance compared to traditional convolution or recurrence-based models in sequence transduction tasks such as machine translation.\n"
     ]
    }
   ],
   "source": [
    "print(ragProcedure(\"how do transformers compare to convolution or recurrence?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
