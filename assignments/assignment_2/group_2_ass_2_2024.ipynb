{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##1.2Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #import the library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The indices 0, 1, 4, 7 have np.nan values.\n"
     ]
    }
   ],
   "source": [
    "#1.\n",
    "f_list = [np.nan , np.nan, \"Austria\", \"Germany\", np.nan, \"Pakistan\", \"np.nan\", np.nan ] # we use 'f_list' as provided in Assignment 2\n",
    "f_list[0] # we check the nan.value\n",
    "# we find indices where np.nan values exist\n",
    "nan_indices = [i for i, value in enumerate(f_list) if isinstance(value, (float,np.floating)) and np.isnan(value)]\n",
    "\n",
    "# Print the result\n",
    "print(f\"The indices {', '.join(map(str, nan_indices))} have np.nan values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#2. Given p2_list\n",
    "p2_list = [ 2 , 3, 4, 5 ]\n",
    "\n",
    "#we replicate 4 times p2_list \n",
    "\n",
    "replicated_p2_list = p2_list * 4\n",
    "print(replicated_p2_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.\n",
    "#We use f_list in #1 and print its length\n",
    "len(f_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My teacher assistant is so boring.\n"
     ]
    }
   ],
   "source": [
    "#4. We use 'text1' as provided in Assignment 2\n",
    "text1 = ['My', 'teacher', 'assistant', 'is', 'so', 'boring.']\n",
    "# We join the elements of the list into a string using join function\n",
    "sentence = ' '.join(text1)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'teacher',\n",
       " 'assistant',\n",
       " 'is',\n",
       " 'so',\n",
       " 'boring.',\n",
       " 'but',\n",
       " 'is',\n",
       " 'very',\n",
       " 'funny.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. Print My TA is so boring, but is very funny. using text1 list\n",
    "#First we extend text 1\n",
    "text1.extend(['but', 'is', 'very', 'funny.'])\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My teacher assistant is so boring. but is very funny.\n"
     ]
    }
   ],
   "source": [
    "# Finally we join the elements of the list into a string\n",
    "sentence = ' '.join(text1)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max value of values1 is 86 and is located in the 0 index.\n",
      "The min value of values1 is 0 and is located in the 7 index.\n"
     ]
    }
   ],
   "source": [
    "#6. Given 'values1'\n",
    "values1 = [ 86, 86, 85, 85, 85, 83, 23, 0, 84, 1 ] \n",
    "#we need to find the max and min values, and its index \n",
    "max_value = max(values1)\n",
    "max_index = values1.index(max_value)\n",
    "\n",
    "min_value = min(values1)\n",
    "min_index = values1.index(min_value)\n",
    "\n",
    "#Finally we print the statement \n",
    "print(f\"The max value of values1 is {max_value} and is located in the {max_index} index.\")\n",
    "print(f\"The min value of values1 is {min_value} and is located in the {min_index} index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last names: ['CORNEJO SANCHEZ', 'ORELLANA QUISPE', 'MORALES CHOQUEHUANCA', 'GUIMARAY RIBEYRO', 'CAMACHO GAVIDIA', 'TINTAYA ORIHUELA', 'CHAVEZ MARTINEZ', 'FIGUEROA MURO', 'GOMEZ CRIBILLERO', 'PALOMINO SEGUÍN', 'LUZON CUEVA', 'SUAÑA ZEGARRA', 'SOTO POMACHAGUA', 'FIORENTINO MARTINEZ', 'LAMA MAVILA', 'MEZA HINOJO', 'LOZADA MURILLO', 'ZAMBRANO JIMENEZ', 'JACOBS LUQUE', 'VIDAL VIDAL', 'TORRES ANICAMA', 'LOPEZ ESTRADA', 'BOYCO ORAMS', 'DIAZ BERROSPI', 'RIEGA ESCALANTE', 'LEVANO TORRES', 'ESQUIVES BRAVO', 'PEREZ GONZALES', 'OTERO MAGUIÑA', 'CLAVO CAMPOS', 'AGUILAR GARCIA', 'CALDAS VELASQUEZ', 'SALAS NUÑEZ BORJA', 'PIZARRO VILLANES', 'QUILLATUPA MORALES', 'HUANCAYA IDONE', 'CALVO PORTOCARRERO', 'IBAÑEZ ABANTO', 'MELÉNDEZ APONTE', 'CRISTIAN SERRANO', 'HINOJOSA CAHUANA', 'ANGLAS GARCÍA', 'ALDAVE ACOSTA', 'NÚÑEZ HUAMÁN', 'OBREGON HUAMAN', 'SOTO PACHERRES', 'INGARUCA RIVERA', 'ROJAS HUAMAN', 'NEYRA SALAS', 'HUERTA ESPINOZA', 'HUANCA MARTINEZ', 'FLORES CADILLO']\n",
      "Names: ['CHRISTIAN SANTOS', 'CRISTIAN NASSER', 'ANGELICA KARINA', 'JOSE ROBERTO', 'ABEL FERNANDO', 'MEIR ALVARO', 'JOSELIN ALEXANDRA', 'LEONEL ARTURO', 'JOSE FELIPE', 'AFRANIA', 'BIANCA MARIETTE', 'ADRIAN ANDRE', 'DORKAS YOMIRA JHERMY', 'LADY ALY', 'HECTOR ANDRE', 'GUSTAVO', 'PERSEO MARCELO', 'MIGUEL ALONZO', 'NICOLAS', 'ROCIO GABRIELA', 'JANE CAMILA', 'MARIA ELISA', 'ALEJANDRO', 'KARLINE ROSMELI', 'STEPHY ROSARIO', 'VALERIA CECILIA', 'SEBASTIAN RENATO', 'JUAN CARLOS', 'MARIANA', 'ANDREA BRIZETH', 'ERICK JOSUE', 'JOSUE DANIEL', 'FABIO MANUEL', 'FERNANDA NICOLLE', 'ANGELA ADELINA', 'CESAR DANTE', 'GABRIELA ISABEL', 'ANGEL MAURICIO', 'JUAN DIEGO', 'ARONE', 'PERCY ALBERTH', 'KEVIN ARTURO', 'CESAR ERNESTO', 'CÉSAR AGUSTO', 'DIANA EDITH', 'RODRIGO FRANCO', 'GRETTEL ALEXANDRA', 'ROSA ANGELA', 'DANTE OMAR', 'YAJAIRA ALEXANDRA', 'JORGE ALBERTO', 'ALEXIS']\n"
     ]
    }
   ],
   "source": [
    "#7. Using 'Last_and_name' list we need to get two separated lists\n",
    "last_and_name = [ \"CORNEJO SANCHEZ, CHRISTIAN SANTOS\", \"ORELLANA QUISPE, CRISTIAN NASSER\", \"MORALES CHOQUEHUANCA, ANGELICA KARINA\", \"GUIMARAY RIBEYRO, JOSE ROBERTO\", \"CAMACHO GAVIDIA, ABEL FERNANDO\", \"TINTAYA ORIHUELA, MEIR ALVARO\", \"CHAVEZ MARTINEZ, JOSELIN ALEXANDRA\", \"FIGUEROA MURO, LEONEL ARTURO\", \"GOMEZ CRIBILLERO, JOSE FELIPE\", \"PALOMINO SEGUÍN, AFRANIA\", \"LUZON CUEVA, BIANCA MARIETTE\", \"SUAÑA ZEGARRA, ADRIAN ANDRE\", \"SOTO POMACHAGUA, DORKAS YOMIRA JHERMY\", \"FIORENTINO MARTINEZ, LADY ALY\", \"LAMA MAVILA, HECTOR ANDRE\", \"MEZA HINOJO, GUSTAVO\", \"LOZADA MURILLO, PERSEO MARCELO\", \"ZAMBRANO JIMENEZ, MIGUEL ALONZO\", \"JACOBS LUQUE, NICOLAS\", \"VIDAL VIDAL, ROCIO GABRIELA\", \"TORRES ANICAMA, JANE CAMILA\", \"LOPEZ ESTRADA, MARIA ELISA\", \"BOYCO ORAMS, ALEJANDRO\", \"DIAZ BERROSPI, KARLINE ROSMELI\", \"RIEGA ESCALANTE, STEPHY ROSARIO\", \"LEVANO TORRES, VALERIA CECILIA\", \"ESQUIVES BRAVO, SEBASTIAN RENATO\", \"PEREZ GONZALES, JUAN CARLOS\", \"OTERO MAGUIÑA, MARIANA\", \"CLAVO CAMPOS, ANDREA BRIZETH\", \"AGUILAR GARCIA, ERICK JOSUE\", \"CALDAS VELASQUEZ, JOSUE DANIEL\", \"SALAS NUÑEZ BORJA, FABIO MANUEL\", \"PIZARRO VILLANES, FERNANDA NICOLLE\", \"QUILLATUPA MORALES, ANGELA ADELINA\", \"HUANCAYA IDONE, CESAR DANTE\", \"CALVO PORTOCARRERO, GABRIELA ISABEL\", \"IBAÑEZ ABANTO, ANGEL MAURICIO\", \"MELÉNDEZ APONTE, JUAN DIEGO\", \"CRISTIAN SERRANO, ARONE\", \"HINOJOSA CAHUANA, PERCY ALBERTH\", \"ANGLAS GARCÍA, KEVIN ARTURO\", \"ALDAVE ACOSTA, CESAR ERNESTO\", \"NÚÑEZ HUAMÁN, CÉSAR AGUSTO\", \"OBREGON HUAMAN, DIANA EDITH\", \"SOTO PACHERRES, RODRIGO FRANCO\", \"INGARUCA RIVERA, GRETTEL ALEXANDRA\", \"ROJAS HUAMAN, ROSA ANGELA\", \"NEYRA SALAS, DANTE OMAR\", \"HUERTA ESPINOZA, YAJAIRA ALEXANDRA\", \"HUANCA MARTINEZ, JORGE ALBERTO\", \"FLORES CADILLO, ALEXIS\" ]\n",
    "\n",
    "#we use map and split functions\n",
    "split_names = list(map(lambda x: x.split(', '), last_and_name))\n",
    "# FInally we get the lists of last names and names\n",
    "last_names = [x[0] for x in split_names]\n",
    "names = [x[1] for x in split_names]\n",
    "\n",
    "print(\"Last names:\", last_names)\n",
    "print(\"Names:\", names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CORNEJO SANCHEZ', 'ORELLANA QUISPE', 'MORALES CHOQUEHUANCA', 'GUIMARAY RIBEYRO', 'CAMACHO GAVIDIA', 'TINTAYA ORIHUELA', 'CHAVEZ MARTINEZ', 'FIGUEROA MURO', 'GOMEZ CRIBILLERO', 'PALOMINO SEGUÍN', 'LUZON CUEVA', 'SUAÑA ZEGARRA', 'SOTO POMACHAGUA', 'FIORENTINO MARTINEZ', 'LAMA MAVILA', 'MEZA HINOJO', 'LOZADA MURILLO', 'ZAMBRANO JIMENEZ', 'JACOBS LUQUE', 'VIDAL VIDAL', 'TORRES ANICAMA', 'LOPEZ ESTRADA', 'BOYCO ORAMS', 'DIAZ BERROSPI', 'RIEGA ESCALANTE', 'LEVANO TORRES', 'ESQUIVES BRAVO', 'PEREZ GONZALES', 'OTERO MAGUIÑA', 'CLAVO CAMPOS', 'AGUILAR GARCIA', 'CALDAS VELASQUEZ', 'SALAS NUÑEZ BORJA', 'PIZARRO VILLANES', 'QUILLATUPA MORALES', 'HUANCAYA IDONE', 'CALVO PORTOCARRERO', 'IBAÑEZ ABANTO', 'MELÉNDEZ APONTE', 'CRISTIAN SERRANO', 'HINOJOSA CAHUANA', 'ANGLAS GARCÍA', 'ALDAVE ACOSTA', 'NÚÑEZ HUAMÁN', 'OBREGON HUAMAN', 'SOTO PACHERRES', 'INGARUCA RIVERA', 'ROJAS HUAMAN', 'NEYRA SALAS', 'HUERTA ESPINOZA', 'HUANCA MARTINEZ', 'FLORES CADILLO']\n"
     ]
    }
   ],
   "source": [
    "#8. given 'emails' and using 'last_names' obtain from #7 we need to get a new list with the last name of students who do not have an email.\n",
    "emails = [\"cscornejo@pucp.edu.pe\", \"orellana.cn@pucp.edu.pe\", \"karina.morales@pucp.edu.pe\", \"a20083223@pucp.pe\", \"abel.camacho@pucp.pe\", \"mtintaya@pucp.edu.pe\", \"joselin.chavez@pucp.edu.pe\", \"a20105737@pucp.pe\", \"jfgomezc@pucp.pe\", \"afrania.palomino@pucp.pe\", \"luzon.bianca@pucp.pe\", \"adrian.suanaz@pucp.pe\", \"soto.y@pucp.edu.pe\", \"a20132766@pucp.pe\", \"andre.lama@pucp.edu.pe\", \"gustavo.meza@pucp.edu.pe\", \"pmlozada@pucp.edu.pe\", \"m.zambranoj@pucp.edu.pe\", \"nicolas.jacobs@pucp.edu.pe\", \"gvidal@pucp.edu.pe\", \"jane.torres@pucp.edu.pe\", \"m.lopez@pucp.edu.pe\", \"alejandro.boyco@pucp.edu.pe\", \"a20167070@pucp.edu.pe\", \"riega.stephy@pucp.edu.pe\", \"vlevanot@pucp.edu.pe\", \"sesquives@pucp.edu.pe\", \"perez.juanc@pucp.edu.pe\", \"mariana.otero@pucp.edu.pe\", \"aclavo@pucp.edu.pe\", \"a20182474@pucp.edu.pe\", \"josue.caldas@pucp.edu.pe\", \"fabio.salas@pucp.edu.pe\", \"fernanda.pizarro@pucp.edu.pe\", \"aquillatupa@pucp.pe\"]\n",
    "\n",
    "# First, we extract the last names from the email addresses\n",
    "email_last_names = list(map(lambda x: x.split('@')[0].split('.')[0], emails))\n",
    "\n",
    "# Finally,we find the students who do not have an email\n",
    "students_without_email = [x for x in last_names if x.lower() not in email_last_names]\n",
    "\n",
    "print(students_without_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         DATE   OPEN   HIGH    LOW  CLOSE\n",
      "0  01/02/1990  17.24  17.24  17.24  17.24\n",
      "1  01/03/1990  18.19  18.19  18.19  18.19\n",
      "2  01/04/1990  19.22  19.22  19.22  19.22\n",
      "3  01/05/1990  20.11  20.11  20.11  20.11\n",
      "4  01/08/1990  20.26  20.26  20.26  20.26\n"
     ]
    }
   ],
   "source": [
    "##1\n",
    "#First, we \"name\" the URL to use it in the pd.read_csv, which is a function that allows us to convert the data in the URL into a DataFrame\n",
    "url='https://raw.githubusercontent.com/datasets/finance-vix/main/data/vix-daily.csv'\n",
    "#Then we use the suggested function to convert the data from the URL into a DataFrame\n",
    "df=pd.read_csv(url)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         DATE   OPEN   HIGH    LOW  CLOSE\n",
      "0  01/02/1990  17.24  17.24  17.24  17.24\n",
      "1  01/03/1990  18.19  18.19  18.19  18.19\n",
      "2  01/04/1990  19.22  19.22  19.22  19.22\n",
      "3  01/05/1990  20.11  20.11  20.11  20.11\n",
      "4  01/08/1990  20.26  20.26  20.26  20.26\n"
     ]
    }
   ],
   "source": [
    "##2.a)\n",
    "#We use the function head to display the first five columns(that is why there is a 5 in the parenthesis)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              DATE         OPEN         HIGH          LOW        CLOSE\n",
      "count         8576  8576.000000  8576.000000  8576.000000  8576.000000\n",
      "unique        8576          NaN          NaN          NaN          NaN\n",
      "top     01/02/1990          NaN          NaN          NaN          NaN\n",
      "freq             1          NaN          NaN          NaN          NaN\n",
      "mean           NaN    19.667087    20.475051    18.915540    19.581101\n",
      "std            NaN     7.979316     8.440179     7.470016     7.906388\n",
      "min            NaN     9.010000     9.310000     8.560000     9.140000\n",
      "25%            NaN    13.937500    14.540000    13.407500    13.880000\n",
      "50%            NaN    17.795000    18.470000    17.220000    17.760000\n",
      "75%            NaN    23.102500    23.960000    22.322500    22.990000\n",
      "max            NaN    82.690000    89.530000    72.760000    82.690000\n"
     ]
    }
   ],
   "source": [
    "##2.b)\n",
    "#Here, we use the function describe to get the summary statistic from the DataFrame. In addition, we use include='all' in case we have no numeric clumns that we would like to display\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DATE   OPEN   HIGH    LOW  CLOSE   LEVEL\n",
      "0     01/02/1990  17.24  17.24  17.24  17.24     Low\n",
      "1     01/03/1990  18.19  18.19  18.19  18.19     Low\n",
      "2     01/04/1990  19.22  19.22  19.22  19.22     Low\n",
      "3     01/05/1990  20.11  20.11  20.11  20.11  Medium\n",
      "4     01/08/1990  20.26  20.26  20.26  20.26  Medium\n",
      "...          ...    ...    ...    ...    ...     ...\n",
      "8571  12/28/2023  12.44  12.65  12.38  12.47     Low\n",
      "8572  12/29/2023  12.55  13.19  12.36  12.45     Low\n",
      "8573  01/02/2024  13.22  14.23  13.10  13.20     Low\n",
      "8574  01/03/2024  13.35  14.22  13.33  14.04     Low\n",
      "8575  01/04/2024  13.93  14.20  13.64  14.13     Low\n",
      "\n",
      "[8576 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "##3.a)\n",
    "#We define the bins and labels\n",
    "bins = [float('-inf'), 20, 30, float('inf')]\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "#We use the function cut to create the new column according with the bins, labels and, obviously, the column LEVEL\n",
    "df['LEVEL'] = pd.cut(df['CLOSE'], bins=bins, labels=labels, right=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           DATE   OPEN   HIGH    LOW  CLOSE  YEAR\n",
      "0    1990-01-02  17.24  17.24  17.24  17.24  1990\n",
      "1    1990-01-03  18.19  18.19  18.19  18.19  1990\n",
      "2    1990-01-04  19.22  19.22  19.22  19.22  1990\n",
      "3    1990-01-05  20.11  20.11  20.11  20.11  1990\n",
      "4    1990-01-08  20.26  20.26  20.26  20.26  1990\n",
      "...         ...    ...    ...    ...    ...   ...\n",
      "8571 2023-12-28  12.44  12.65  12.38  12.47  2023\n",
      "8572 2023-12-29  12.55  13.19  12.36  12.45  2023\n",
      "8573 2024-01-02  13.22  14.23  13.10  13.20  2024\n",
      "8574 2024-01-03  13.35  14.22  13.33  14.04  2024\n",
      "8575 2024-01-04  13.93  14.20  13.64  14.13  2024\n",
      "\n",
      "[8576 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "##3.b)\n",
    "#Here, to create the new column, we should convert the first column to datetime, as the function says\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "#Once that is done, all we have to do is to use the following function to extract the year from the date\n",
    "df['YEAR'] = df['DATE'].dt.year\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m new_YEAR_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2024\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Then, to add a row containing this values, we should use loc function\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(df)] \u001b[38;5;241m=\u001b[39m [pd\u001b[38;5;241m.\u001b[39mto_datetime(new_DATE), new_CLOSE_value, new_LEVEL_value, new_YEAR_value]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[1;32mc:\\Users\\jpc04\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:849\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    848\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 849\u001b[0m iloc\u001b[38;5;241m.\u001b[39m_setitem_with_indexer(indexer, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\jpc04\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1825\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1822\u001b[0m     indexer, missing \u001b[38;5;241m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[0;32m   1824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[1;32m-> 1825\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer_missing(indexer, value)\n\u001b[0;32m   1826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1829\u001b[0m     \u001b[38;5;66;03m# must come after setting of missing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jpc04\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:2158\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   2155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_list_like_indexer(value):\n\u001b[0;32m   2156\u001b[0m         \u001b[38;5;66;03m# must have conforming columns\u001b[39;00m\n\u001b[0;32m   2157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[1;32m-> 2158\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot set a row with mismatched columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2160\u001b[0m     value \u001b[38;5;241m=\u001b[39m Series(value, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns, name\u001b[38;5;241m=\u001b[39mindexer)\n\u001b[0;32m   2162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj):\n\u001b[0;32m   2163\u001b[0m     \u001b[38;5;66;03m# We will ignore the existing dtypes instead of using\u001b[39;00m\n\u001b[0;32m   2164\u001b[0m     \u001b[38;5;66;03m#  internals.concat logic\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "##4\n",
    "#First, we should create the values that seem appropiate to us\n",
    "new_DATE = '2024-01-01'\n",
    "new_CLOSE_value = 22\n",
    "new_LEVEL_value = 'Medium'\n",
    "new_YEAR_value = 2024\n",
    "#Then, to add a row containing this values, we should use loc function\n",
    "df.loc[len(df)] = [pd.to_datetime(new_DATE), new_CLOSE_value, new_LEVEL_value, new_YEAR_value]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR\n",
      "1990    23.063478\n",
      "1991    18.373373\n",
      "1992    15.452047\n",
      "1993    12.686245\n",
      "1994    13.925516\n",
      "1995    12.388770\n",
      "1996    16.442165\n",
      "1997    22.363984\n",
      "1998    25.602976\n",
      "1999    24.371912\n",
      "2000    23.315000\n",
      "2001    25.749677\n",
      "2002    27.292460\n",
      "2003    21.982857\n",
      "2004    15.479644\n",
      "2005    12.807063\n",
      "2006    12.810320\n",
      "2007    17.535936\n",
      "2008    32.695534\n",
      "2009    31.479008\n",
      "2010    22.548889\n",
      "2011    24.202579\n",
      "2012    17.801640\n",
      "2013    14.230119\n",
      "2014    14.175992\n",
      "2015    16.674087\n",
      "2016    15.825635\n",
      "2017    11.090239\n",
      "2018    16.639841\n",
      "2019    15.387540\n",
      "2020    29.251304\n",
      "2021    19.656151\n",
      "2022    25.637148\n",
      "2023    16.849027\n",
      "2024    13.790000\n",
      "Name: CLOSE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "##5\n",
    "# Converting the 'Date' column to datetime format\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "# Now, as we have been asked, we must group by 'Year' and then calculate the average of the 'Close' column\n",
    "average_close_by_year = df.groupby('YEAR')['CLOSE'].mean()\n",
    "\n",
    "print(average_close_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'LEVEL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jpc04\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\jpc04\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jpc04\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'LEVEL'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##5b\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# To count the number of 'High', 'Medium', and 'Low' Level days from the column Level\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m LEVEL_counts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLEVEL\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(LEVEL_counts)\n",
      "File \u001b[1;32mc:\\Users\\jpc04\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\jpc04\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'LEVEL'"
     ]
    }
   ],
   "source": [
    "##5b\n",
    "# To count the number of 'High', 'Medium', and 'Low' Level days from the column Level\n",
    "LEVEL_counts = df['LEVEL'].value_counts()\n",
    "\n",
    "print(LEVEL_counts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
