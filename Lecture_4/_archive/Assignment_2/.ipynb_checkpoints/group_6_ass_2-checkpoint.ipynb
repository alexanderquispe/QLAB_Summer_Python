{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their variables and values labels from this path `\"../../_data/endes/2019\"`. The name of imported files should be named as `rec_1`, `rec_2` and `rec_3` for files `REC0111.sav`, `RE223132.sav` and `RE516171.sav` respectively. The name of the variable and value labels should be `var_labels1` and `value_labels1` for `rec1`, `var_labels2` and `value_labels2` for `rec2`, and `var_labels3` and `value_labels3` for `rec3`. **Hint: See the section 3.3.4 of [the lecture 3](https://github.com/alexanderquispe/Diplomado_PUCP/blob/main/Lecture_3/Lecture_3.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyreadstat\n",
    "# pip install savReaderWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Guillermo\\\\Documents\\\\GitHub\\\\Diplomado_PUCP\\\\Lecture_4\\\\Assignment_2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_1 = pd.read_spss( r\"../../_data/endes/2019/REC0111.sav\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_2 = pd.read_spss( r\"../../_data/endes/2019/RE223132.sav\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_3 = pd.read_spss( r\"../../_data/endes/2019/RE516171.sav\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import savReaderWriter as sav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sav.SavHeaderReader( r\"../../_data/endes/2019/REC0111.sav\", ioUtf8=True) as header:\n",
    "    metadata = header.all()\n",
    "    var_labels1 = metadata.varLabels\n",
    "    value_labels1 = metadata.valueLabels\n",
    "    rec_1.attrs[ 'var_labels1' ] = var_labels1\n",
    "    rec_1.attrs[ 'value_labels1' ] = value_labels1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sav.SavHeaderReader( r\"../../_data/endes/2019/REC0111.sav\", ioUtf8=True) as header:\n",
    "    metadata = header.all()\n",
    "    var_labels2 = metadata.varLabels\n",
    "    value_labels2 = metadata.valueLabels\n",
    "    rec_2.attrs[ 'var_labels2' ] = var_labels2   \n",
    "    rec_2.attrs[ 'value_labels2' ] = value_labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sav.SavHeaderReader( r\"../../_data/endes/2019/REC0111.sav\", ioUtf8=True) as header:\n",
    "    metadata = header.all()\n",
    "    var_labels3 = metadata.varLabels\n",
    "    value_labels3 = metadata.valueLabels \n",
    "    rec_3.attrs[ 'var_labels3' ] = var_labels3  \n",
    "    rec_3.attrs[ 'value_labels3' ] = value_labels3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select the following columns for each data set:\n",
    "|Data|Columns|\n",
    "|---|---|\n",
    "|rec1| CASEID, V000, V001, V002, V003, V004, V007, V008, V009, V010, V011, V012, V024, V102, V120, V121, V122, V123, V124, V125, V127, V133 |\n",
    "|rec2| CASEID, V201, V2018, V301, V302, ..., V323, V323A, V325A, V326, V327, V337, V359, V360, V361, V362, V363, V364, V367, V372, V372A, V375A, V376, V376A, V379, V380 |\n",
    "|rec3| CASEID, V501, V502, V503, V504, V505, V506, V507, V508, V509, V510, V511, V512, V513, V525, V613, V714, V715 |\n",
    "\n",
    "\n",
    "Additioanlly, you should update the variables and value labels objects. They must have information only for the selected columns. The new dataframes should be name as `rec1_1`, `rec2_1`, and `rec3_1`. The new varible labels objects should be named as `new_var_labels1`, `new_var_labels2`, and `new_var_labels3`. The new value labels objects should be named as `new_value_labels1`, `new_value_labels2`, and `new_value_labels3` **Hint: Use the `loc` and column names to filter, `for loop`,   and [this link](https://stackoverflow.com/questions/3420122/filter-dict-to-contain-only-certain-keys) to update the var and value dictionary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selcol_rec_1=\"CASEID\", \"V000\", \"V001\", \"V002\", \"V003\", \"V004\", \"V007\", \"V008\", \"V009\", \"V010\", \"V011\", \"V012\", \"V024\", \"V102\", \"V120\", \"V121\", \"V122\", \"V123\",\"V124\", \"V125\", \"V127\", \"V133\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selcol_rec_2=\"CASEID\", \"V201\", \"V218\", \"V301\", \"V302\", \"V323\", \"V323A\", \"V325A\", \"V326\", \"V327\", \"V337\", \"V359\", \"V360\", \"V361\", \"V362\", \"V363\", \"V364\", \"V367\", \"V372\", \"V372A\", \"V375A\", \"V376\", \"V376A\", \"V379\", \"V380\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "selcol_rec_3=\"CASEID\", \"V501\", \"V502\", \"V503\", \"V504\", \"V505\", \"V506\", \"V507\", \"V508\", \"V509\", \"V510\", \"V511\", \"V512\", \"V513\", \"V525\", \"V613\", \"V714\", \"V715\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec1_1 = rec_1.loc[ :, selcol_rec_1]\n",
    "rec2_1 = rec_2.loc[ :, selcol_rec_2]\n",
    "rec3_1 = rec_3.loc[ :, selcol_rec_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_var_labels1 = { key: var_labels1[ key ] for key in selcol_rec_1 }\n",
    "new_value_labels1 = { key: value_labels1[ key ] for key in selcol_rec_1 if key in value_labels1.keys() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'V201'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-590a773419f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_var_labels2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvar_labels2\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mselcol_rec_2\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnew_value_labels2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue_labels2\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mselcol_rec_2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue_labels2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-590a773419f7>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_var_labels2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvar_labels2\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mselcol_rec_2\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnew_value_labels2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue_labels2\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mselcol_rec_2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue_labels2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'V201'"
     ]
    }
   ],
   "source": [
    "new_var_labels2 = { key: var_labels2[ key ] for key in selcol_rec_2 }\n",
    "new_value_labels2 = { key: value_labels2[ key ] for key in selcol_rec_2 if key in value_labels2.keys() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'V501'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-702b2f5937a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_var_labels3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvar_labels3\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mselcol_rec_3\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnew_value_labels3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue_labels3\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mselcol_rec_3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue_labels3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-702b2f5937a8>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_var_labels3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvar_labels3\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mselcol_rec_3\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnew_value_labels3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue_labels3\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mselcol_rec_3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue_labels3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'V501'"
     ]
    }
   ],
   "source": [
    "new_var_labels3 = { key: var_labels3[ key ] for key in selcol_rec_3 }\n",
    "new_value_labels3 = { key: value_labels3[ key ] for key in selcol_rec_3 if key in value_labels3.keys() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate a new column for `rec1_1` named as `year`. It should be equal to `2019`. Also, you must update this new variable for the `var_labels` dictionary. Generate a new key for `new_var_labels1` and the value for this key should be **\"Year of the survey\"** **Hint: Use `loc` and `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec1_1['year'] = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_var_labels1['year'] = \"Year of the survey\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Merge `rec1_1`, `rec2_1`, and `rec3_1` using **CASEID**. Name this new object as `endes_2019`. **Hint: Use [this link](https://stackoverflow.com/questions/53645882/pandas-merging-101)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_endes_2019 = rec1_1.merge(     # dataframe A to be merged\n",
    "                 rec2_1,           # dataframe B to be merged with\n",
    "                 on = 'CASEID',    # by variable name\n",
    "                 how = 'left',     # keep A and complete with B\n",
    "                 validate = \"1:1\"  # Asign unique values (others: m:1, 1:m, m:m)\n",
    "                 )\n",
    "\n",
    "endes_2019    =  sub_endes_2019.merge(# dataframe A to be merged\n",
    "                 rec3_1,              # dataframe B to be merged with\n",
    "                 on = 'CASEID',       # by variable name\n",
    "                 how = 'left',        # keep A and complete with B\n",
    "                 validate = \"1:1\"     # Asign unique values (others: m:1, 1:m, m:m)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Unify all the `new_var_labels` in one object and `new_value_labels` in another one object. Name these two objects as `var_labels` and `value_labels`. Use them to generate new attributes for `endes_2019`. These attributes should be named as `var_labels` and `value_labels`. Your final output should be equal to `endes_2019_ta` object **Hint: Use `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base labels\n",
    "new_var_labels = new_var_labels1.copy()\n",
    "new_value_labels = new_value_labels1.copy()\n",
    "\n",
    "# Adding labels2\n",
    "new_var_labels.update( new_var_labels2 )\n",
    "new_value_labels.update( new_value_labels2 )\n",
    "\n",
    "# Adding labels3\n",
    "new_var_labels.update( new_var_labels3 )\n",
    "new_value_labels.update( new_value_labels3 )\n",
    "\n",
    "# Final labels\n",
    "var_labels = new_var_labels.copy()\n",
    "value_labels = new_value_labels.copy()\n",
    "\n",
    "# Generating atributes for endes_2019\n",
    "endes_2019.attrs[ 'var_labels' ] = var_labels\n",
    "endes_2019.attrs[ 'value_labels' ] = value_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if current output is similar to `endes_2019_ta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urllib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-24d3000882bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Importing endes_2019_ta data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'https://www.dropbox.com/s/gwcssinb1j9zr6s/endes_2019_ta.pkl?dl=1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mendes_2019_ta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m## Validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'urllib' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing endes_2019_ta data\n",
    "output = urllib.request.urlopen(r'https://www.dropbox.com/s/gwcssinb1j9zr6s/endes_2019_ta.pkl?dl=1')\n",
    "endes_2019_ta = pickle.load( output )\n",
    "\n",
    "## Validation\n",
    "# Variables validation\n",
    "var_validation = endes_2019.attrs[ 'var_labels' ] == endes_2019_ta.attrs[ 'var_labels' ]\n",
    "print('Are var_labels the same for endes_2019 and endes_2019_ta? \\t', var_validation)\n",
    "# Values validation\n",
    "value_validation = endes_2019.attrs[ 'value_labels' ] == endes_2019_ta.attrs[ 'value_labels' ]\n",
    "print('Are value_labels the same for endes_2019 and endes_2019_ta? \\t', value_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now, replicate your code of the prevoius sections but for years **2019, 2018, 2017, 2016, 2015**. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their **variables and values labels** from this path `\"../../_data/endes/\"`. For this excersie you must use a for loop. This loop must iterate over **2019, 2018, 2017, 2016, 2015 folders** and import these files. All the files have the same name. You must store these files and their labels in a nested dictionary named as `all_data`. The keys of the dictionary should be named as `year_2019`, for example, and the keys of the nested dictionary should be `data`, `var_labels`, and `value_labels`. The output of this exercise should be equal to the `all_data_ta` object. **Hint: Use [this link](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=4d6de78e00e7001f16bf6473c2eb7ce24fb611cd&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f616c6578616e6465727175697370652f4469706c6f6d61646f5f505543502f346436646537386530306537303031663136626636343733633265623763653234666236313163642f4c6563747572655f342f4c6563747572655f342e6970796e62&logged_in=true&nwo=alexanderquispe%2FDiplomado_PUCP&path=Lecture_4%2FLecture_4.ipynb&platform=windows&repository_id=427747212&repository_type=Repository&version=95#4.2.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we are at rec_1 data of 2015\n",
      " we are at rec_2 data of 2015\n",
      " we are at rec_3 data of 2015\n",
      " we are at year column part of 2015\n",
      " we are at merge part of 2015\n",
      " we are at labeling part of 2015\n",
      " we are at storing data part of 2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f7e873d2f334>:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rec_1_selec['year'] = year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we are at rec_1 data of 2016\n",
      " we are at rec_2 data of 2016\n",
      " we are at rec_3 data of 2016\n",
      " we are at year column part of 2016\n",
      " we are at merge part of 2016\n",
      " we are at labeling part of 2016\n",
      " we are at storing data part of 2016\n",
      " we are at rec_1 data of 2017\n",
      " we are at rec_2 data of 2017\n",
      " we are at rec_3 data of 2017\n",
      " we are at year column part of 2017\n",
      " we are at merge part of 2017\n",
      " we are at labeling part of 2017\n",
      " we are at storing data part of 2017\n",
      " we are at rec_1 data of 2018\n",
      " we are at rec_2 data of 2018\n",
      " we are at rec_3 data of 2018\n",
      " we are at year column part of 2018\n",
      " we are at merge part of 2018\n",
      " we are at labeling part of 2018\n",
      " we are at storing data part of 2018\n",
      " we are at rec_1 data of 2019\n",
      " we are at rec_2 data of 2019\n",
      " we are at rec_3 data of 2019\n",
      " we are at year column part of 2019\n",
      " we are at merge part of 2019\n",
      " we are at labeling part of 2019\n",
      " we are at storing data part of 2019\n"
     ]
    }
   ],
   "source": [
    "all_data = {} # empty object to store results, in for loop\n",
    "\n",
    "## Defining lists of variables\n",
    "# List 1\n",
    "var_list_1 = [\"CASEID\", \"V000\", \"V001\", \"V002\", \"V003\",\n",
    "              \"V004\", \"V007\", \"V008\", \"V009\", \"V010\",\n",
    "              \"V011\", \"V012\", \"V024\", \"V102\",\"V120\",\n",
    "              \"V121\", \"V122\", \"V123\", \"V124\", \"V125\",\n",
    "              \"V127\", \"V133\"]\n",
    "\n",
    "# List 2    \n",
    "rec_2_aux = pd.read_spss( r\"../../_data/endes/2019/RE223132.sav\" )\n",
    "\n",
    "var_list_2 = [\"CASEID\", \"V201\", \"V218\", \"V301\"] \\\n",
    "             + rec_2_aux.loc[ : , \"V302\":\"V323\"].columns.tolist() \\\n",
    "             + [\"V323A\", \"V325A\", \"V326\", \"V327\",\n",
    "              \"V337\", \"V359\", \"V360\", \"V361\", \"V362\",\n",
    "              \"V363\", \"V364\", \"V367\", \"V372\", \"V372A\",\n",
    "              \"V375A\", \"V376\", \"V376A\", \"V379\", \"V380\"]\n",
    "\n",
    "#List 3\n",
    "var_list_3 = [\"CASEID\", \"V501\", \"V502\", \"V503\",\"V504\",\n",
    "              \"V505\", \"V506\", \"V507\", \"V508\", \"V509\",\n",
    "              \"V510\", \"V511\", \"V512\", \"V513\", \"V525\",\n",
    "              \"V613\", \"V714\", \"V715\"]    \n",
    "\n",
    "## library for labeling with sav\n",
    "import savReaderWriter as sav\n",
    "\n",
    "for year in range( 2015, 2020 ):\n",
    "    \n",
    "    ## REC_1 data\n",
    "    # reading REC_1 data\n",
    "    rec_1_data = pd.read_spss( fr\"../../_data/endes/{year}/REC0111.sav\" )\n",
    "    \n",
    "    # getting labels \n",
    "    with sav.SavHeaderReader( fr\"../../_data/endes/{year}/REC0111.sav\", ioUtf8=True) as header:\n",
    "        metadata = header.all()\n",
    "        var_label1 = metadata.varLabels.copy() #use .copy to don´t drag the same object by iteration\n",
    "        value_label1 = metadata.valueLabels.copy()\n",
    "    \n",
    "    # getting the selected variables new data frame\n",
    "    if year == 2018: # V007 not foun in 2018 year, provisional solution with if statement\n",
    "        var_list_1 = [\"CASEID\", \"V000\", \"V001\", \"V002\", \"V003\",\n",
    "                      \"V004\", \"V008\", \"V009\", \"V010\",\n",
    "                      \"V011\", \"V012\", \"V024\", \"V102\",\"V120\",\n",
    "                      \"V121\", \"V122\", \"V123\", \"V124\", \"V125\",\n",
    "                      \"V127\", \"V133\"]\n",
    "    else:\n",
    "        var_list_1 = var_list_1\n",
    "        \n",
    "    rec_1_selec = rec_1_data[var_list_1]\n",
    "    \n",
    "    \n",
    "    # Labels\n",
    "    new_var_label1 = {} #empty objtects to store \"values\" from loop\n",
    "    new_value_label1 = {}\n",
    "\n",
    "    for key in var_list_1: # Loop by list\n",
    "        if key in var_label1: # Store if select var label exist\n",
    "            new_var_label1[f'{key}'] = var_label1[f'{key}']\n",
    "\n",
    "        if key in value_label1:# Store if select value label exist\n",
    "            new_value_label1[f'{key}'] = value_label1[f'{key}']\n",
    "    \n",
    "    # Print for error detection\n",
    "    print(f\" we are at rec_1 data of {year}\")\n",
    "    \n",
    "    \n",
    "    ## REC_2 data\n",
    "    # reading REC_2 data\n",
    "    rec_2_data = pd.read_spss( fr\"../../_data/endes/{year}/RE223132.sav\" )\n",
    "    \n",
    "    # getting labels \n",
    "    with sav.SavHeaderReader( fr\"../../_data/endes/{year}/RE223132.sav\", ioUtf8=True) as header:\n",
    "        metadata = header.all()\n",
    "        var_label2 = metadata.varLabels.copy() #use .copy to don´t drag the same object by iteration\n",
    "        value_label2 = metadata.valueLabels.copy()\n",
    "    \n",
    "    # getting the selected variables new data frame\n",
    "    rec_2_selec = rec_2_data[var_list_2]\n",
    "    \n",
    "    \n",
    "    # Labels\n",
    "    new_var_label2 = {} #empty objtects to store \"values\" from loop\n",
    "    new_value_label2 = {}\n",
    "\n",
    "    for key in var_list_2: # Loop by list\n",
    "        if key in var_label2: # Store if select var label exist\n",
    "            new_var_label2[f'{key}'] = var_label2[f'{key}']\n",
    "\n",
    "        if key in value_label2:# Store if select value label exist\n",
    "            new_value_label2[f'{key}'] = value_label2[f'{key}']\n",
    "    \n",
    "    # Print for error detection\n",
    "    print(f\" we are at rec_2 data of {year}\")\n",
    "    \n",
    "    \n",
    "    ## REC_3 data\n",
    "    # reading REC_3 data\n",
    "    rec_3_data = pd.read_spss( fr\"../../_data/endes/{year}/RE516171.sav\" )\n",
    "    \n",
    "    # getting labels \n",
    "    with sav.SavHeaderReader( fr\"../../_data/endes/{year}/RE516171.sav\", ioUtf8=True) as header:\n",
    "        metadata = header.all()\n",
    "        var_label3 = metadata.varLabels.copy() #use .copy to don´t drag the same object by iteration\n",
    "        value_label3 = metadata.valueLabels.copy()\n",
    "    \n",
    "    # getting the selected variables new data frame\n",
    "    rec_3_selec = rec_3_data[var_list_3]\n",
    "    \n",
    "    \n",
    "    # Labels\n",
    "    new_var_label3 = {} #empty objtects to store \"values\" from loop\n",
    "    new_value_label3 = {}\n",
    "\n",
    "    for key in var_list_3: # Loop by list\n",
    "        if key in var_label3: # Store if select var label exist\n",
    "            new_var_label3[f'{key}'] = var_label3[f'{key}']\n",
    "\n",
    "        if key in value_label3:# Store if select value label exist\n",
    "            new_value_label3[f'{key}'] = value_label3[f'{key}']\n",
    "            \n",
    "    # Print for error detection\n",
    "    print(f\" we are at rec_3 data of {year}\")\n",
    "    \n",
    "    ## Year column and label\n",
    "    # year column\n",
    "    rec_1_selec['year'] = year\n",
    "    # year variable label\n",
    "    new_var_label1['year'] = \"Year of the survey\"\n",
    "    # Print for error detection\n",
    "    print(f\" we are at year column part of {year}\")\n",
    "    \n",
    "    \n",
    "    ## Data merge\n",
    "    sub_endes  = rec_1_selec.merge(     # dataframe A to be merged\n",
    "                 rec_2_selec,           # dataframe B to be merged with\n",
    "                 on = 'CASEID',         # by variable name\n",
    "                 how = 'left',          # keep A and complete with B\n",
    "                 validate = \"1:1\"       # Asign unique values (others: m:1, 1:m, m:m)\n",
    "                 )\n",
    "\n",
    "    endes     =  sub_endes.merge(     # dataframe A to be merged\n",
    "                 rec_3_selec,              # dataframe B to be merged with\n",
    "                 on = 'CASEID',       # by variable name\n",
    "                 how = 'left',        # keep A and complete with B\n",
    "                 validate = \"1:1\"     # Asign unique values (others: m:1, 1:m, m:m)\n",
    "                 )\n",
    "    # Print for error detection\n",
    "    print(f\" we are at merge part of {year}\")\n",
    "    \n",
    "    \n",
    "    ## Labeling data\n",
    "    # Base labels\n",
    "    new_var_label = new_var_label1.copy()\n",
    "    new_value_label = new_value_label1.copy()\n",
    "\n",
    "    # Adding labels2\n",
    "    new_var_label.update( new_var_label2 )\n",
    "    new_value_label.update( new_value_label2 )\n",
    "\n",
    "    # Adding labels3\n",
    "    new_var_label.update( new_var_label3 )\n",
    "    new_value_label.update( new_value_label3 )\n",
    "\n",
    "    # Final labels\n",
    "    var_labels = new_var_label.copy()\n",
    "    value_labels = new_value_label.copy()\n",
    "    \n",
    "    # Print for error detection\n",
    "    print(f\" we are at labeling part of {year}\")\n",
    "\n",
    "    ## Storing data\n",
    "    store_values = {\"data\" : endes , \n",
    "                      \"var_labels\" : var_labels, \n",
    "                      \"value_labels\" : value_labels\n",
    "                    }\n",
    "    all_data[ f'year_{year}'] = store_values\n",
    "    # Print for error detection\n",
    "    print(f\" we are at storing data part of {year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if current output is similar to `all_data_ta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncommon keys between all_data and all_data_ta:  {}\n"
     ]
    }
   ],
   "source": [
    "# Importinga all_data_ta\n",
    "output = urllib.request.urlopen(r'https://www.dropbox.com/s/dqv1d9of2uo6hul/all_data_ta.pkl?dl=1')\n",
    "all_data_ta = pickle.load( output )\n",
    "\n",
    "# Test if all_data is iqual to all_data_ta\n",
    "# We are going to compare by keys, as long there are too much commons keys\n",
    "# let's test if there are any uncommon keys\n",
    "uncommon_pairs = dict()\n",
    "for key in all_data:\n",
    "    if (key in all_data_ta and all_data[key] != all_data[key]):\n",
    "        uncommon_pairs[key] = all_data[key]\n",
    "print('Uncommon keys between all_data and all_data_ta: ',uncommon_pairs)\n",
    "# If you look for common key pairs you can try it by uncoment the next code, \n",
    "# this gives us a print of all keys, this is similar to print data_all or data_all_ta:\n",
    "#common_pairs = dict()\n",
    "#for key in all_data:\n",
    "#    if (key in all_data_ta and all_data[key] == all_data[key]):\n",
    "#        common_pairs[key] = all_data[key]\n",
    "#print('common keys between all_data and all_data_ta: ',common_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use `all_data` to append all the data sets. Store all data sets in a list using `for loop`. Then, use `pd.concat` to append all the data sets. Also, you must reset the index to have a good-looking data. This new object should be named as `endes_data_2015_2019`. **Hint: Use [this code](https://stackoverflow.com/questions/32444138/concatenate-a-list-of-pandas-dataframes-together)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Store all the `var_labels` and `value_labels` in a dictionary named as `all_var_labels` and `all_value_labels`. The first keys should be the year for both dictionaries.Then, use them to generate new attributes for `endes_data_2015_2019`. These attributes should be named as `var_labels` and `value_labels`. Your final output should be equal to `endes_data_2015_2019_ta` object. **Hint: Use [this link](https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=4d6de78e00e7001f16bf6473c2eb7ce24fb611cd&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f616c6578616e6465727175697370652f4469706c6f6d61646f5f505543502f346436646537386530306537303031663136626636343733633265623763653234666236313163642f4c6563747572655f342f4c6563747572655f342e6970796e62&logged_in=true&nwo=alexanderquispe%2FDiplomado_PUCP&path=Lecture_4%2FLecture_4.ipynb&platform=windows&repository_id=427747212&repository_type=Repository&version=95#4.2.3.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = urllib.request.urlopen( r'https://www.dropbox.com/s/oifvyjrr7s0e1md/endes_data_2015_2019_ta.pkl?dl=1')\n",
    "endes_data_2015_2019_ta = pickle.load( output )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Use `endes_data_2015_2019` data to generate a new object named `mean_key_vars` to find the mean of **total children ever born (V201)**, **Ideal number of children (V613)**, **Husbands education-single yrs (V715)**, and **Age at first marriage (V511)** by year and department **(V024)**. Name these columns as **mean_total_children, mean_ideal_children, mean_hb_yr_educ and mean_first_marriage**, respectively. **Hint: Use groupby and [this link](https://stackoverflow.com/questions/40901770/is-there-a-simple-way-to-change-a-column-of-yes-no-to-1-0-in-a-pandas-dataframe).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Merge `mean_key_vars` with `endes_data_2015_2019`. Name this object `final_result`. Your ouput should be equal to `final_result_ta`. **Hint: Use merge.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = urllib.request.urlopen( r'https://www.dropbox.com/s/ntor3jbpdxpj9u1/final_result_ta.pkl?dl=1' )\n",
    "final_result_ta = pickle.load( output )"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
